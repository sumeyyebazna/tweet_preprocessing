{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd831411",
   "metadata": {},
   "source": [
    "# Tweet Preprocessing Pipeline – Full Version\n",
    "Bu notebook, Sentiment140 veri kümesi üzerinde 11 aşamalı NLP ön işleme sürecini adım adım içermektedir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed0665",
   "metadata": {},
   "source": [
    "## Adım 0 – Veri Kümesini Yükleme ve İlk 100 Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2209547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/content/drive/MyDrive/training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1', header=None)\n",
    "df.columns = ['Sentiment', 'id', 'date', 'query', 'user', 'review']\n",
    "df = df[['review']].head(100).copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84af0b01",
   "metadata": {},
   "source": [
    "## Adım 1 – Küçük Harfe Çevirme (Lowercasing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab91d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lowercase'] = df['review'].str.lower()\n",
    "df[['review', 'lowercase']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c793c981",
   "metadata": {},
   "source": [
    "## Adım 2 – HTML Temizleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47496ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html(text):\n",
    "    return re.sub(r'<.*?>', '', text)\n",
    "df['no_html'] = df['lowercase'].apply(remove_html)\n",
    "df[['lowercase', 'no_html']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e336a5a",
   "metadata": {},
   "source": [
    "## Adım 3 – URL Temizleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d48867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+', '', text)\n",
    "df['no_urls'] = df['no_html'].apply(remove_urls)\n",
    "df[['no_html', 'no_urls']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd88f2",
   "metadata": {},
   "source": [
    "## Adım 4 – Noktalama Temizleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bcae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "df['no_punct'] = df['no_urls'].apply(remove_punctuation)\n",
    "df[['no_urls', 'no_punct']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e73e73",
   "metadata": {},
   "source": [
    "## Adım 5 – ChatWords Dönüştürme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aedd887",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words_map = {\n",
    "    \"u\": \"you\", \"r\": \"are\", \"idk\": \"i don't know\", \"lol\": \"laughing\",\n",
    "    \"btw\": \"by the way\", \"omg\": \"oh my god\", \"brb\": \"be right back\"\n",
    "}\n",
    "def convert_chat_words(text):\n",
    "    words = text.split()\n",
    "    return ' '.join([chat_words_map.get(word, word) for word in words])\n",
    "df['chat_normalized'] = df['no_punct'].apply(convert_chat_words)\n",
    "df[['no_punct', 'chat_normalized']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e17907",
   "metadata": {},
   "source": [
    "## Adım 6 – Yazım Düzeltme (İlk 10 Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc24953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob --quiet\n",
    "from textblob import TextBlob\n",
    "def correct_spelling(text):\n",
    "    return str(TextBlob(text).correct())\n",
    "df.loc[:9, 'corrected'] = df.loc[:9, 'chat_normalized'].apply(correct_spelling)\n",
    "df[['chat_normalized', 'corrected']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e9bd82",
   "metadata": {},
   "source": [
    "## Adım 7 – Stopword Temizleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5992335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    words = text.split()\n",
    "    return ' '.join([word for word in words if word not in stop_words])\n",
    "df['no_stopwords'] = df['corrected'].apply(remove_stopwords)\n",
    "df[['corrected', 'no_stopwords']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d749822b",
   "metadata": {},
   "source": [
    "## Adım 8 – Emoji Temizleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a373afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji --quiet\n",
    "import emoji\n",
    "def remove_emojis(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "df['no_emoji'] = df['no_stopwords'].apply(remove_emojis)\n",
    "df[['no_stopwords', 'no_emoji']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c999c2d",
   "metadata": {},
   "source": [
    "## Adım 9 – Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac6a1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize(text):\n",
    "    try:\n",
    "        return word_tokenize(str(text))\n",
    "    except:\n",
    "        return []\n",
    "df['tokens'] = df['no_emoji'].apply(tokenize)\n",
    "df[['no_emoji', 'tokens']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7557218",
   "metadata": {},
   "source": [
    "## Adım 10 – Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5c4f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(word) for word in tokens]\n",
    "df['stemmed'] = df['tokens'].apply(stem_tokens)\n",
    "df[['tokens', 'stemmed']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5fe7d3",
   "metadata": {},
   "source": [
    "## Adım 11 – Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "df['lemmatized'] = df['tokens'].apply(lemmatize_tokens)\n",
    "df[['tokens', 'lemmatized']].head(10)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
